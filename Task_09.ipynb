{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134e7f9d",
   "metadata": {},
   "source": [
    "# Hello, KAN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf5cd0",
   "metadata": {},
   "source": [
    "### Kolmogorov-Arnold representation theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e5321",
   "metadata": {},
   "source": [
    "Kolmogorov-Arnold representation theorem states that if $f$ is a multivariate continuous function\n",
    "on a bounded domain, then it can be written as a finite composition of continuous functions of a\n",
    "single variable and the binary operation of addition. More specifically, for a smooth $f : [0,1]^n \\to \\mathbb{R}$,\n",
    "\n",
    "\n",
    "$$f(x) = f(x_1,...,x_n)=\\sum_{q=1}^{2n+1}\\Phi_q(\\sum_{p=1}^n \\phi_{q,p}(x_p))$$\n",
    "\n",
    "where $\\phi_{q,p}:[0,1]\\to\\mathbb{R}$ and $\\Phi_q:\\mathbb{R}\\to\\mathbb{R}$. In a sense, they showed that the only true multivariate function is addition, since every other function can be written using univariate functions and sum. However, this 2-Layer width-$(2n+1)$ Kolmogorov-Arnold representation may not be smooth due to its limited expressive power. We augment its expressive power by generalizing it to arbitrary depths and widths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8766a",
   "metadata": {},
   "source": [
    "### Kolmogorov-Arnold Network (KAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3b1ee",
   "metadata": {},
   "source": [
    "The Kolmogorov-Arnold representation can be written in matrix form\n",
    "\n",
    "$$f(x)={\\bf \\Phi}_{\\rm out}\\circ{\\bf \\Phi}_{\\rm in}\\circ {\\bf x}$$\n",
    "\n",
    "where \n",
    "\n",
    "$${\\bf \\Phi}_{\\rm in}= \\begin{pmatrix} \\phi_{1,1}(\\cdot) & \\cdots & \\phi_{1,n}(\\cdot) \\\\ \\vdots & & \\vdots \\\\ \\phi_{2n+1,1}(\\cdot) & \\cdots & \\phi_{2n+1,n}(\\cdot) \\end{pmatrix},\\quad {\\bf \\Phi}_{\\rm out}=\\begin{pmatrix} \\Phi_1(\\cdot) & \\cdots & \\Phi_{2n+1}(\\cdot)\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6521452",
   "metadata": {},
   "source": [
    "We notice that both ${\\bf \\Phi}_{\\rm in}$ and ${\\bf \\Phi}_{\\rm out}$ are special cases of the following function matrix ${\\bf \\Phi}$ (with $n_{\\rm in}$ inputs, and $n_{\\rm out}$ outputs), we call a Kolmogorov-Arnold layer:\n",
    "\n",
    "$${\\bf \\Phi}= \\begin{pmatrix} \\phi_{1,1}(\\cdot) & \\cdots & \\phi_{1,n_{\\rm in}}(\\cdot) \\\\ \\vdots & & \\vdots \\\\ \\phi_{n_{\\rm out},1}(\\cdot) & \\cdots & \\phi_{n_{\\rm out},n_{\\rm in}}(\\cdot) \\end{pmatrix}$$\n",
    "\n",
    "${\\bf \\Phi}_{\\rm in}$ corresponds to $n_{\\rm in}=n, n_{\\rm out}=2n+1$, and ${\\bf \\Phi}_{\\rm out}$ corresponds to $n_{\\rm in}=2n+1, n_{\\rm out}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b410498",
   "metadata": {},
   "source": [
    "After defining the layer, we can construct a Kolmogorov-Arnold network simply by stacking layers! Let's say we have $L$ layers, with the $l^{\\rm th}$ layer ${\\bf \\Phi}_l$ have shape $(n_{l+1}, n_{l})$. Then the whole network is\n",
    "\n",
    "$${\\rm KAN}({\\bf x})={\\bf \\Phi}_{L-1}\\circ\\cdots \\circ{\\bf \\Phi}_1\\circ{\\bf \\Phi}_0\\circ {\\bf x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbde9a",
   "metadata": {},
   "source": [
    "In constrast, a Multi-Layer Perceptron is interleaved by linear layers ${\\bf W}_l$ and nonlinearities $\\sigma$:\n",
    "\n",
    "$${\\rm MLP}({\\bf x})={\\bf W}_{L-1}\\circ\\sigma\\circ\\cdots\\circ {\\bf W}_1\\circ\\sigma\\circ {\\bf W}_0\\circ {\\bf x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f7795",
   "metadata": {},
   "source": [
    "A KAN can be easily visualized. (1) A KAN is simply stack of KAN layers. (2) Each KAN layer can be visualized as a fully-connected layer, with a 1D function placed on each edge. Let's see an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb5f75",
   "metadata": {},
   "source": [
    "### Get started with KANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571d531",
   "metadata": {},
   "source": [
    "Initialize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2075ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# create a KAN: 2D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5).\n",
    "model = KAN(width=[784,20,10], grid=3, k=3, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72e076",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88048b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([64, 784])\n",
      "Batch of labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 1: Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the images to 1D\n",
    "])\n",
    "\n",
    "# Step 2: Load MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Step 3: Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 4: Verify the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")  # Should be [64, 1, 28, 28]\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")  # Should be [64]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c63fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    images = images.view(images.size(0), -1)  # Flatten the images (28x28 -> 784)\n",
    "    print(images.shape)  # Should be [batch_size, 784]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fdb05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x213db11cda0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fbd5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000, 28, 28]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data.shape, train_loader.dataset.targets.shape, test_loader.dataset.data.shape, test_loader.dataset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce512283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]),\n",
       " torch.Size([60000]),\n",
       " tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198, 198,\n",
       "          198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254, 225,\n",
       "          254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,  14,\n",
       "           67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "          221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 203,\n",
       "          254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 254,\n",
       "          254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224, 254,\n",
       "          115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254, 254,\n",
       "           52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254, 254,\n",
       "           52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254, 219,\n",
       "           40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,  18,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "        dtype=torch.uint8),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data.shape, train_loader.dataset.targets.shape, test_loader.dataset.data[0], test_loader.dataset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401d7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the MNIST images\n",
    "flattened_data = train_loader.dataset.data.view(train_loader.dataset.data.size(0), -1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4421eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the flattened data\n",
    "normalized_data = (flattened_data / 255.0)  # Normalize to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ea3231",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7526400000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pass the normalized data to the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\kan\\MultKAN.py:819\u001b[39m, in \u001b[36mMultKAN.forward\u001b[39m\u001b[34m(self, x, singularity_avoiding, y_th)\u001b[39m\n\u001b[32m    816\u001b[39m x = \u001b[38;5;28mself\u001b[39m.subnode_scale[l][\u001b[38;5;28;01mNone\u001b[39;00m,:] * x + \u001b[38;5;28mself\u001b[39m.subnode_bias[l][\u001b[38;5;28;01mNone\u001b[39;00m,:]\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_act:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     postacts = \u001b[43mpostacts_numerical\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostacts_symbolic\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;66;03m# self.neurons_scale.append(torch.mean(torch.abs(x), dim=0))\u001b[39;00m\n\u001b[32m    822\u001b[39m     \u001b[38;5;66;03m#grid_reshape = self.act_fun[l].grid.reshape(self.width_out[l + 1], self.width_in[l], -1)\u001b[39;00m\n\u001b[32m    823\u001b[39m     input_range = torch.std(preacts, dim=\u001b[32m0\u001b[39m) + \u001b[32m0.1\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7526400000 bytes."
     ]
    }
   ],
   "source": [
    "# Pass the normalized data to the model\n",
    "output = model(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08447583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46717e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 2]), torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from kan.utils import create_dataset\n",
    "# # create dataset f(x,y) = exp(sin(pi*x)+y^2)\n",
    "# f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)\n",
    "# dataset = create_dataset(f, n_var=2, device=device)\n",
    "# dataset['train_input'].shape, dataset['train_label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5b85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6add1d",
   "metadata": {},
   "source": [
    "Plot KAN at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot KAN at initialization\n",
    "# model(dataset['train_input']);\n",
    "# model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f51f1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 28 is out of bounds for dimension 0 with size 28",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# plot KAN at initialization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[32m      3\u001b[39m model.plot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\pykan\\kan\\MultKAN.py:782\u001b[39m, in \u001b[36mMultKAN.forward\u001b[39m\u001b[34m(self, x, singularity_avoiding, y_th)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, singularity_avoiding=\u001b[38;5;28;01mFalse\u001b[39;00m, y_th=\u001b[32m10.\u001b[39m):\n\u001b[32m    749\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[33;03m    forward pass\u001b[39;00m\n\u001b[32m    751\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m \u001b[33;03m    >>> print(model(x, singularity_avoiding=True, y_th=1.))\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m x.shape[\u001b[32m1\u001b[39m] == \u001b[38;5;28mself\u001b[39m.width_in[\u001b[32m0\u001b[39m]\n\u001b[32m    785\u001b[39m     \u001b[38;5;66;03m# cache data\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: index 28 is out of bounds for dimension 0 with size 28"
     ]
    }
   ],
   "source": [
    "# plot KAN at initialization\n",
    "model(train_loader.dataset.data);\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf67e30",
   "metadata": {},
   "source": [
    "Train KAN with sparsity regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97111d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.97e-02 | test_loss: 1.96e-02 | reg: 5.75e+00 | : 100%|█| 50/50 [00:24<00:00,  2.08it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=50, lamb=0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30c3ab",
   "metadata": {},
   "source": [
    "Plot trained KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434c5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot KAN at initialization\n",
    "# model(dataset['train_input']);\n",
    "# model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a4f67a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "model hasn't seen any data yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\pykan\\kan\\MultKAN.py:1068\u001b[39m, in \u001b[36mMultKAN.plot\u001b[39m\u001b[34m(self, folder, beta, metric, scale, tick, sample, in_vars, out_vars, title, varscale)\u001b[39m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.acts == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1067\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_data == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mmodel hasn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mt seen any data yet.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28mself\u001b[39m.forward(\u001b[38;5;28mself\u001b[39m.cache_data)\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric == \u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mException\u001b[39m: model hasn't seen any data yet."
     ]
    }
   ],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576856cf",
   "metadata": {},
   "source": [
    "Prune KAN and replot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe6fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.prune()\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08ad99",
   "metadata": {},
   "source": [
    "Continue training and replot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a2db11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.77e-02 | test_loss: 1.70e-02 | reg: 8.37e+00 | : 100%|█| 50/50 [00:07<00:00,  7.09it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.3\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, opt=\"LBFGS\", steps=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8768d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.4\n"
     ]
    }
   ],
   "source": [
    "model = model.refine(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f73098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 4.62e-04 | test_loss: 4.70e-04 | reg: 8.34e+00 | : 100%|█| 50/50 [00:08<00:00,  5.74it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, opt=\"LBFGS\", steps=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35d505",
   "metadata": {},
   "source": [
    "Automatically or manually set activation functions to be symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c0642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing (0,0,0) with sin, r2=0.9999999192750186, c=2\n",
      "fixing (0,1,0) with x^2, r2=0.9999999835650273, c=2\n",
      "fixing (1,0,0) with exp, r2=0.9999999909570408, c=2\n",
      "saving model version 0.6\n"
     ]
    }
   ],
   "source": [
    "mode = \"auto\" # \"manual\"\n",
    "\n",
    "if mode == \"manual\":\n",
    "    # manual mode\n",
    "    model.fix_symbolic(0,0,0,'sin');\n",
    "    model.fix_symbolic(0,1,0,'x^2');\n",
    "    model.fix_symbolic(1,0,0,'exp');\n",
    "elif mode == \"auto\":\n",
    "    # automatic mode\n",
    "    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']\n",
    "    model.auto_symbolic(lib=lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ba616",
   "metadata": {},
   "source": [
    "Continue training till machine precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0800415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.88e-13 | test_loss: 1.98e-13 | reg: 0.00e+00 | : 100%|█| 50/50 [00:02<00:00, 18.10it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, opt=\"LBFGS\", steps=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39da499",
   "metadata": {},
   "source": [
    "Obtain the symbolic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf44f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.0 e^{1.0 x_{2}^{2} + 1.0 \\sin{\\left(3.1416 x_{1} \\right)}}$"
      ],
      "text/plain": [
       "1.0*exp(1.0*x_2**2 + 1.0*sin(3.1416*x_1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kan.utils import ex_round\n",
    "\n",
    "ex_round(model.symbolic_formula()[0][0],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e635f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.09MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.32MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.60MB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'interp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m train_loader, test_loader = get_mnist_loaders()\n\u001b[32m     75\u001b[39m kan_model = KAN().to(device)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mtrain_kan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m test_kan(kan_model, test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_kan\u001b[39m\u001b[34m(model, train_loader, epochs, lr)\u001b[39m\n\u001b[32m     49\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m     50\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     53\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mKAN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     36\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspline1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mSplineActivation.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterp\u001b[49m(x, \u001b[38;5;28mself\u001b[39m.knots, \u001b[38;5;28mself\u001b[39m.values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch\\__init__.py:2681\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2681\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'interp'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "def get_mnist_loaders(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_set = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Define a Spline Activation Function\n",
    "class SplineActivation(nn.Module):\n",
    "    def __init__(self, num_knots=10):\n",
    "        super().__init__()\n",
    "        self.knots = nn.Parameter(torch.linspace(-1, 1, num_knots))\n",
    "        self.values = nn.Parameter(torch.randn(num_knots))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.interp(x, self.knots, self.values)\n",
    "\n",
    "# Define the KAN Model\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.spline1 = SplineActivation()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.spline1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training Function\n",
    "def train_kan(model, train_loader, epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "def test_kan(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_mnist_loaders()\n",
    "kan_model = KAN().to(device)\n",
    "train_kan(kan_model, train_loader)\n",
    "test_kan(kan_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a20777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7853\n",
      "Epoch 2, Loss: 0.6973\n",
      "Epoch 3, Loss: 0.6967\n",
      "Epoch 4, Loss: 0.7673\n",
      "Epoch 5, Loss: 0.8591\n",
      "Test Accuracy: 65.86%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "def get_mnist_loaders(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_set = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Define a Spline Activation Function\n",
    "class SplineActivation(nn.Module):\n",
    "    def __init__(self, num_knots=10):\n",
    "        super().__init__()\n",
    "        self.knots = nn.Parameter(torch.linspace(-1, 1, num_knots))  # Knot positions\n",
    "        self.values = nn.Parameter(torch.randn(num_knots))  # Values at knots\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform linear interpolation manually\n",
    "        x_clamped = torch.clamp(x, min=self.knots[0].item(), max=self.knots[-1].item())\n",
    "        indices = torch.searchsorted(self.knots, x_clamped) - 1\n",
    "        indices = torch.clamp(indices, 0, len(self.knots) - 2)\n",
    "        \n",
    "        x0 = self.knots[indices]\n",
    "        x1 = self.knots[indices + 1]\n",
    "        y0 = self.values[indices]\n",
    "        y1 = self.values[indices + 1]\n",
    "        \n",
    "        # Linear interpolation\n",
    "        t = (x_clamped - x0) / (x1 - x0)\n",
    "        return y0 + t * (y1 - y0)\n",
    "\n",
    "# Define the KAN Model\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.spline1 = SplineActivation()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.spline1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training Function\n",
    "def train_kan(model, train_loader, epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "def test_kan(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_mnist_loaders()\n",
    "kan_model = KAN().to(device)\n",
    "train_kan(kan_model, train_loader)\n",
    "test_kan(kan_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSOC25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
