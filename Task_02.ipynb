{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II: Classical Graph Neural Network (GNN) \n",
    "## For Task II, you will use ParticleNetâ€™s data for Quark/Gluon jet classification available [here](https://zenodo.org/records/3164691#.YigdGt9MHrB) with its corresponding description. \n",
    "<ul>\n",
    "    <li> Choose 2 Graph-based architectures of your choice to classify jets as being quarks or gluons. Provide a description on what considerations you have taken to project this point-cloud dataset to a set of interconnected nodes and edges.\n",
    "    <li> Discuss the resulting performance of the 2 chosen architectures. \n",
    "</ul>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downloading and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'y']\n",
      "Item:\n",
      "X\n",
      "Data[Item]\n",
      "Dimension: 3\n",
      "[[[ 6.21580243e-01 -8.49013586e-01  5.02133119e+00 -2.11000000e+02]\n",
      "  [ 6.41751806e-01 -1.25072426e+00  5.44050034e+00  2.11000000e+02]\n",
      "  [ 2.01318448e-01 -9.03872384e-01  4.90318847e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 3.56561010e-01  1.28450604e+00  1.74690075e+00  2.20000000e+01]\n",
      "  [ 1.64376978e-01  1.34179128e+00  1.62318700e+00  2.20000000e+01]\n",
      "  [ 4.29715184e+00  1.00013501e+00  1.88439142e+00  1.30000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.08500898e+01  1.05767017e+00  6.35388683e+00 -2.11000000e+02]\n",
      "  [ 4.13482156e-02  7.46948660e-01  6.13804598e+00  2.20000000e+01]\n",
      "  [ 5.16705560e-01  1.14248976e+00  6.33461836e+00 -2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.65098368e-01 -6.30028387e-01  6.58745282e+00  2.20000000e+01]\n",
      "  [ 5.82179999e-01 -5.93970848e-01  5.93824615e+00  2.11000000e+02]\n",
      "  [ 8.70825542e-01 -6.85617630e-01  5.97198619e+00 -2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 2.94381463e-01 -7.25735015e-01  5.50873569e+00  2.20000000e+01]\n",
      "  [ 1.34087798e+00 -7.29881843e-01  5.50888453e+00 -3.21000000e+02]\n",
      "  [ 6.32790901e+00 -7.15341122e-01  5.51705428e+00  1.30000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 2.65977826e+00  8.02494340e-01  3.20426987e+00  2.11000000e+02]\n",
      "  [ 1.81350610e+00  7.50877903e-01  3.26367324e+00  2.20000000e+01]\n",
      "  [ 1.53513343e-01  1.32260575e+00  3.45010304e+00  2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Shape:\n",
      "(100000, 134, 4)\n",
      "Item:\n",
      "y\n",
      "Data[Item]\n",
      "Dimension: 1\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "Shape:\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "data = load('ParticleNet_jetClassification\\\\QG_jets_1.npz')\n",
    "lst = data.files\n",
    "print (lst)\n",
    "for item in lst:\n",
    "    print(\"Item:\")\n",
    "    print(item)\n",
    "    print (\"Data[Item]\")\n",
    "    print (f\"Dimension: {np.ndim(data[item])}\")\n",
    "    print(data[item])\n",
    "    print (\"Shape:\")\n",
    "    print (data[item].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'y']\n",
      "Item:\n",
      "X\n",
      "Data[Item]\n",
      "Dimension: 3\n",
      "[[[ 3.93203966e-01  3.50858538e-01  8.55396185e-01  2.20000000e+01]\n",
      "  [ 2.46338260e+00  1.65702215e-01  7.61525307e-01  2.20000000e+01]\n",
      "  [ 2.56550934e+00 -5.68774857e-02  7.89741416e-01  2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.61694863e+00 -1.31621717e+00  9.98804499e-01 -2.11000000e+02]\n",
      "  [ 1.58885285e-01 -1.51184963e+00  9.30179606e-01  2.20000000e+01]\n",
      "  [ 1.91738285e+00 -1.84739871e+00  1.31081899e+00 -3.21000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 9.02373553e-01  1.26997967e+00  3.01591217e+00  3.21000000e+02]\n",
      "  [ 7.52902861e-02  9.00624519e-01  3.43656410e+00  2.20000000e+01]\n",
      "  [ 5.31198316e-01  1.16975047e+00  3.28990387e+00  2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.42832333e-02 -2.46208974e-01  2.17259328e+00  2.20000000e+01]\n",
      "  [ 1.32675426e-01 -4.28298145e-01  2.85323286e+00  2.20000000e+01]\n",
      "  [ 1.20109483e-01 -4.96853995e-01  2.19714075e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 6.64028114e-01 -2.48153084e-01  3.81776810e+00 -2.11000000e+02]\n",
      "  [ 6.14624836e-01 -3.50465095e-01  3.80802685e+00  2.11000000e+02]\n",
      "  [ 1.56733660e+00 -4.72029324e-01  3.17529917e+00  2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 4.45111252e-01  6.86026633e-01  5.62889137e-02  2.20000000e+01]\n",
      "  [ 2.21785352e-01  3.34816960e-01  5.70924630e-01  2.20000000e+01]\n",
      "  [ 1.47769453e+00  7.42232672e-02 -1.18604209e-02 -2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Shape:\n",
      "(100000, 132, 4)\n",
      "Item:\n",
      "y\n",
      "Data[Item]\n",
      "Dimension: 1\n",
      "[1. 1. 1. ... 1. 1. 0.]\n",
      "Shape:\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "data = load('ParticleNet_jetClassification\\\\QG_jets_2.npz')\n",
    "lst = data.files\n",
    "print (lst)\n",
    "for item in lst:\n",
    "    print(\"Item:\")\n",
    "    print(item)\n",
    "    print (\"Data[Item]\")\n",
    "    print (f\"Dimension: {np.ndim(data[item])}\")\n",
    "    print(data[item])\n",
    "    print (\"Shape:\")\n",
    "    print (data[item].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'y']\n",
      "Item:\n",
      "X\n",
      "Data[Item]\n",
      "Dimension: 3\n",
      "[[[ 4.07201237e+00  1.09956405e+00  3.85282264e+00  2.11000000e+02]\n",
      "  [ 7.73380167e-01  7.13987483e-01  3.68545357e+00 -2.11000000e+02]\n",
      "  [ 1.14809578e-01  5.78924014e-01  3.74063251e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 2.39305753e+00  1.12300153e+00  6.36019987e+00 -2.11000000e+02]\n",
      "  [ 2.96587205e+00  8.59178845e-01  6.37095677e+00  2.11000000e+02]\n",
      "  [ 4.78190822e+00  7.45407187e-01  6.31373908e+00  1.30000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 6.74573195e-01  1.02391768e+00  2.14886551e+00  2.11000000e+02]\n",
      "  [ 3.37872916e+00  5.89779017e-01  2.07253851e+00 -2.21200000e+03]\n",
      "  [ 1.56140940e+00  1.00366747e+00  2.25537923e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.96117783e+00  5.69513955e-01  1.43561827e+00  2.20000000e+01]\n",
      "  [ 1.76964996e+00 -1.71362996e-02  1.74397455e+00 -2.11000000e+02]\n",
      "  [ 6.10189208e+00  1.21070340e-02  1.75584304e+00  3.21000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 6.15951895e-01 -9.18019797e-01  1.24508523e+00  2.20000000e+01]\n",
      "  [ 5.08260657e+00 -1.57286044e+00  1.11430625e+00  3.21000000e+02]\n",
      "  [ 7.46450907e+00 -1.55764155e+00  1.05315625e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.48603101e-01 -1.03610904e+00  4.30775965e+00  2.20000000e+01]\n",
      "  [ 3.69540769e-01 -9.40924431e-01  4.65645666e+00  2.11000000e+02]\n",
      "  [ 6.78110748e-01 -1.54770870e+00  4.86939633e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Shape:\n",
      "(100000, 128, 4)\n",
      "Item:\n",
      "y\n",
      "Data[Item]\n",
      "Dimension: 1\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "Shape:\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "data = load('ParticleNet_jetClassification\\\\QG_jets_3.npz')\n",
    "lst = data.files\n",
    "print (lst)\n",
    "for item in lst:\n",
    "    print(\"Item:\")\n",
    "    print(item)\n",
    "    print (\"Data[Item]\")\n",
    "    print (f\"Dimension: {np.ndim(data[item])}\")\n",
    "    print(data[item])\n",
    "    print (\"Shape:\")\n",
    "    print (data[item].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'y']\n",
      "Item:\n",
      "X\n",
      "Data[Item]\n",
      "Dimension: 3\n",
      "[[[ 9.86502732e-01 -1.35386410e+00  2.64458483e-01  2.20000000e+01]\n",
      "  [ 1.05703896e+00 -1.34115771e+00  2.19554508e-01  2.21200000e+03]\n",
      "  [ 6.21758834e-01 -1.29731387e+00  2.74126482e-01  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 2.25822279e+00 -8.77790734e-01  4.93749866e+00 -2.11000000e+02]\n",
      "  [ 1.05114111e+01 -1.06668930e+00  5.34817384e+00  2.11200000e+03]\n",
      "  [ 4.82717145e+00 -1.06442943e+00  5.25285614e+00 -2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.39930755e-01  1.25928787e+00  1.70230491e+00  2.20000000e+01]\n",
      "  [ 6.17137998e-01  1.64929615e+00  1.96346964e+00  2.20000000e+01]\n",
      "  [ 5.70673135e-01  1.84701626e+00  1.89809982e+00 -2.11000000e+02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.45050443e+00  8.93212584e-01  2.75104519e+00 -2.11000000e+02]\n",
      "  [ 6.18814307e-02  2.97050993e-01  2.42893994e+00  2.11000000e+02]\n",
      "  [ 8.55867110e-01  7.21385896e-01  2.97491929e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 3.42364412e+00  1.91124060e+00  3.57532950e+00  2.11000000e+02]\n",
      "  [ 1.66703991e-01  1.62315844e+00  3.04744259e+00  2.20000000e+01]\n",
      "  [ 2.82207572e+00  1.89861351e+00  3.30448989e+00  2.21200000e+03]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.40541704e+00 -8.05061377e-01  1.79024133e+00 -2.11000000e+02]\n",
      "  [ 5.57270930e-01 -2.58368281e-01  1.29400867e+00  2.11000000e+02]\n",
      "  [ 1.21089110e+00 -8.88822296e-01  1.48453429e+00  2.20000000e+01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Shape:\n",
      "(100000, 134, 4)\n",
      "Item:\n",
      "y\n",
      "Data[Item]\n",
      "Dimension: 1\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Shape:\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "data = load('ParticleNet_jetClassification\\\\QG_jets_withbc_0.npz')\n",
    "lst = data.files\n",
    "print (lst)\n",
    "for item in lst:\n",
    "    print(\"Item:\")\n",
    "    print(item)\n",
    "    print (\"Data[Item]\")\n",
    "    print (f\"Dimension: {np.ndim(data[item])}\")\n",
    "    print(data[item])\n",
    "    print (\"Shape:\")\n",
    "    print (data[item].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN - Code, Explaination and Performamce Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading and loading data to train a **Graph Neural Network (GCN)**, for quark/gluon jet classification, several considerations must be taken into account to ensure the data is properly prepared and suitable for training. Below are the key considerations:\n",
    "\n",
    "### **1. Dataset Format**\n",
    "- **Structure**:\n",
    "  - The dataset should be structured in a way that supports graph representation.\n",
    "  - For your dataset:\n",
    "    - **Node Features (`X`)**: Each particle in a jet is represented as a node with features like `pt`, `rapidity`, `azimuthal angle`, and `pdgid`.\n",
    "    - **Labels (`y`)**: Each jet (graph) is labeled as `0` (gluon) or `1` (quark).\n",
    "- **File Format**:\n",
    "  - The dataset is stored in an `.npz` file, which is a compressed format for NumPy arrays. This is efficient for storing large datasets.\n",
    "\n",
    "### **2. Data Preprocessing**\n",
    "- **Zero-Padding**:\n",
    "  - Since jets have varying numbers of particles, the dataset is padded to a maximum multiplicity (`M`) to ensure uniform shape. (This is where we use the information that if there are non-uniform number of particles, we will be padding the nest with 0s till we match the size M)\n",
    "  - Consideration: Ensure that padded particles (zero-particles) are excluded or handled appropriately during graph construction. (We need to ensure that we handle the 0s appropriately to ensure model's accuracy and general nature)\n",
    "\n",
    "- **Normalization**\n",
    "\n",
    "- **Edge Definition**\n",
    "\n",
    "### **3. Graph Representation**\n",
    "- **Nodes**:\n",
    "  - Each particle (of M) in a jet is treated as a node, with its features (`pt`, `rapidity`, `azimuthal angle`, `pdgid`) as the node attributes.\n",
    "- **Edges**:\n",
    "  - A fully connected graph is created for simplicity, where all particles are connected to each other.\n",
    "  - Consideration: Fully connected graphs may introduce noise. Using physics-based edges (e.g., proximity in Î”R space) can improve performance.\n",
    "- **Graph-Level Labels**:\n",
    "  - Each graph (jet) is assigned a single label (`0` for gluon, `1` for quark).\n",
    "\n",
    "### **4. Data Splitting**\n",
    "- **Training, Validation, and Test Sets**:\n",
    "  - The dataset should be split into training, validation, and test sets to evaluate the model's performance.\n",
    "  - Consideration: Ensure the splits are stratified to maintain the balance between quark and gluon jets in each set.\n",
    "\n",
    "### **5. Data Loading**\n",
    "- **Batching**:\n",
    "  - Use a `DataLoader` to batch multiple graphs together for efficient training.\n",
    "  - Consideration: Ensure the `batch_size` is appropriate for your system's memory.\n",
    "- **Shuffling**:\n",
    "  - Shuffle the dataset during training to prevent the model from learning the order of the data.\n",
    "\n",
    "### **6. Computational Considerations**\n",
    "- **Memory Usage**:\n",
    "  - Large datasets with fully connected graphs can consume significant memory. Consider reducing the number of edges or using sparse representations.\n",
    "- **GPU Acceleration**:\n",
    "  - If using a GPU, ensure the data is moved to the GPU (`.to(device)`) for faster computation.\n",
    "\n",
    "### **7. Handling Imbalanced Data**\n",
    "- **Class Balance**:\n",
    "  - The dataset is balanced (50% quark jets, 50% gluon jets), so no additional steps are needed for class balancing.\n",
    "- **Class Weights**:\n",
    "  - If the dataset were imbalanced, we could use class weights in the loss function to handle the imbalance.\n",
    "\n",
    "### **8. Data Integrity**\n",
    "- **Validation**:\n",
    "  - Ensure the dataset is free of errors, such as:\n",
    "    - Missing or corrupted data.\n",
    "    - Incorrect labels.\n",
    "  - Example: Verify that padded particles have zero features and are excluded during graph construction.\n",
    "\n",
    "### **9. Scalability**\n",
    "- **Large Datasets**:\n",
    "  - If the dataset is too large to fit into memory, we can consider:\n",
    "    - Loading the data in chunks.\n",
    "    - Using a streaming data pipeline.\n",
    "\n",
    "### Summary of Considerations\n",
    "| **Aspect**              | **Consideration**                                                                 |\n",
    "|--------------------------|-----------------------------------------------------------------------------------|\n",
    "| Dataset Format           | Ensure the dataset supports graph representation (nodes, edges, labels).         |\n",
    "| Preprocessing            | Normalize features, handle zero-padded particles, and define meaningful edges.   |\n",
    "| Graph Representation     | Use nodes for particles, edges for relationships, and graph-level labels.        |\n",
    "| Data Splitting           | Split into training, validation, and test sets with balanced classes.            |\n",
    "| Data Loading             | Use batching and shuffling for efficient training.                               |\n",
    "| Memory Usage             | Optimize graph size and batch size for memory constraints.                       |\n",
    "| Physics-Based Features   | Use domain knowledge to define features and edges (e.g., Î”R, energy thresholds). |\n",
    "| Class Balance            | Ensure balanced classes or use class weights if imbalanced.                      |\n",
    "| Data Integrity           | Validate the dataset for missing or corrupted data.                              |\n",
    "| Scalability              | Handle large datasets with chunking or streaming.                                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GCN is working on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Overview**\n",
    "The dataset consists of:\n",
    "1. **Graph Description**:\n",
    "   - The entire graph is treated as either a gluon jet or a quark jet.\n",
    "    - **Nodes**: Particles in the jet.\n",
    "    - **Edges**: Relationships between particles (e.g., fully connected or based on physics-based criteria like Î”R).\n",
    "\n",
    "2. **Node Features (`X`)**:\n",
    "   - Shape: `(100000, M, 4)` where:\n",
    "     - `100000`: Number of jets.\n",
    "     - `M`: Maximum number of particles in a jet (zero-padded).\n",
    "     - `4`: Features per particle (`pt`, `rapidity`, `azimuthal angle`, `pdgid`).\n",
    "   - Each particle in a jet is treated as a **node** in the graph.\n",
    "\n",
    "3. **Labels (`y`)**:\n",
    "   - Shape: `(100000,)` where each jet is labeled as:\n",
    "     - `0`: Gluon jet.\n",
    "     - `1`: Quark jet.\n",
    "   - Each jet is treated as a **graph**, and the task is **graph-level classification**.\n",
    "     \n",
    "\n",
    "### **1. Graph Construction**\n",
    "Each jet is converted into a graph using the `torch_geometric.data.Data` class:\n",
    "- **Nodes**: Represent particles in the jet, with features like `pt`, `rapidity`, `azimuthal angle`, and `pdgid`.\n",
    "- **Edges**: A fully connected graph is created for simplicity (all particles are connected to each other). This is defined by the `create_edge_index` function.\n",
    "\n",
    "For example:\n",
    "- A jet with 3 particles (`M=3`) will have:\n",
    "  - **Nodes**: 3 nodes, each with 4 features.\n",
    "  - **Edges**: Fully connected graph with 6 edges (excluding self-loops).\n",
    "\n",
    "### **2. Node Feature Transformation (GCN Layers)**\n",
    "The GCN model consists of two **GCNConv** layers. These layers perform the following operations:\n",
    "\n",
    "#### **Step 1: First GCN Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.conv1(x, edge_index)\n",
    "x = F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - `x`: Node features (shape: `[num_nodes, input_dim]`).\n",
    "  - `edge_index`: Graph structure (edges).\n",
    "- **Operation**:\n",
    "  - Each node aggregates features from its neighbors (defined by `edge_index`) and transforms them using a learnable weight matrix.\n",
    "  - The result is passed through a ReLU activation function to introduce non-linearity.\n",
    "- **Output**:\n",
    "  - Updated node features (shape: `[num_nodes, hidden_dim]`).\n",
    "\n",
    "#### **Step 2: Second GCN Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.conv2(x, edge_index)\n",
    "x = F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - Node features from the first GCN layer.\n",
    "- **Operation**:\n",
    "  - Similar to the first layer, but transforms the features into a new space.\n",
    "- **Output**:\n",
    "  - Final node features (shape: `[num_nodes, hidden_dim]`).\n",
    "\n",
    "### **3. Graph-Level Pooling**\n",
    "After the GCN layers, the node features are aggregated into a single graph-level representation using **global mean pooling**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = global_mean_pool(x, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - `x`: Node features (shape: `[num_nodes, hidden_dim]`).\n",
    "  - `batch`: A tensor that maps each node to its corresponding graph in the batch.\n",
    "- **Operation**:\n",
    "  - For each graph, the mean of the node features is computed.\n",
    "- **Output**:\n",
    "  - Graph-level features (shape: `[batch_size, hidden_dim]`).\n",
    "\n",
    "### **4. Graph-Level Classification**\n",
    "The graph-level features are passed through a fully connected layer to predict the class (quark or gluon):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.fc(x)\n",
    "return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - Graph-level features (shape: `[batch_size, hidden_dim]`).\n",
    "- **Operation**:\n",
    "  - The fully connected layer maps the graph-level features to the output dimension (`output_dim=2` for binary classification).\n",
    "  - The `log_softmax` function converts the output into log probabilities for each class.\n",
    "- **Output**:\n",
    "  - Predicted probabilities for each graph in the batch (shape: `[batch_size, 2]`).\n",
    "\n",
    "### **5. Training**\n",
    "During training, the model learns to minimize the **cross-entropy loss** between the predicted probabilities and the true labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(out, batch.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - `out`: Predicted probabilities (shape: `[batch_size, 2]`).\n",
    "  - `batch.y`: True labels for each graph in the batch (shape: `[batch_size]`).\n",
    "- **Operation**:\n",
    "  - The loss function computes how far the predictions are from the true labels.\n",
    "- **Output**:\n",
    "  - A scalar loss value, which is used to update the model parameters.\n",
    "\n",
    "### **6. Evaluation**\n",
    "During evaluation, the model predicts the class for each graph and computes the accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = out.max(dim=1)\n",
    "correct += pred.eq(batch.y).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`out.max(dim=1)`**: Gets the predicted class (in this case: Binary Classification -> Gluon jet or Quark jet) (index of the maximum probability).\n",
    "- **`pred.eq(batch.y)`**: Compares predictions with true labels.\n",
    "- **Accuracy**: The ratio of correct predictions to the total number of graphs.\n",
    "\n",
    "### **Summary of Workflow**\n",
    "1. **Graph Construction**:\n",
    "   - Each jet is represented as a graph with particles as nodes and edges connecting them.\n",
    "2. **Node Feature Transformation**:\n",
    "   - GCN layers aggregate and transform node features based on the graph structure.\n",
    "3. **Graph-Level Pooling**:\n",
    "   - Node features are aggregated into a single graph-level representation.\n",
    "4. **Classification**:\n",
    "   - The graph-level features are used to predict the class (quark or gluon).\n",
    "5. **Training**:\n",
    "   - The model learns by minimizing the cross-entropy loss.\n",
    "6. **Evaluation**:\n",
    "   - The model's performance is evaluated using accuracy.\n",
    "\n",
    "### Why GCN Works for This Dataset\n",
    "- **Particle Interactions**: The GCN captures relationships between particles in a jet through the graph structure. (Since we are connecting all the paritlces as either a fully connected graph or a physics-based critera)\n",
    "- **Feature Aggregation**: The GCN layers aggregate particle-level features to learn meaningful jet-level representations. (The properties of all the particles measured are considered to predict the type of jet)\n",
    "- **Graph-Level Pooling**: The pooling operation ensures that the model outputs a single prediction for each jet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6141, Accuracy: 0.6756\n",
      "Epoch 1, Loss: 0.6041, Accuracy: 0.6745\n",
      "Epoch 2, Loss: 0.6039, Accuracy: 0.6853\n",
      "Epoch 3, Loss: 0.6035, Accuracy: 0.6838\n",
      "Epoch 4, Loss: 0.6031, Accuracy: 0.6813\n",
      "Epoch 5, Loss: 0.6021, Accuracy: 0.6770\n",
      "Epoch 6, Loss: 0.6032, Accuracy: 0.6543\n",
      "Epoch 7, Loss: 0.6035, Accuracy: 0.6721\n",
      "Epoch 8, Loss: 0.6028, Accuracy: 0.6698\n",
      "Epoch 9, Loss: 0.6028, Accuracy: 0.6705\n",
      "Epoch 10, Loss: 0.6024, Accuracy: 0.6729\n",
      "Epoch 11, Loss: 0.6033, Accuracy: 0.6739\n",
      "Epoch 12, Loss: 0.6028, Accuracy: 0.6756\n",
      "Epoch 13, Loss: 0.6033, Accuracy: 0.6849\n",
      "Epoch 14, Loss: 0.6038, Accuracy: 0.6822\n",
      "Epoch 15, Loss: 0.6037, Accuracy: 0.6845\n",
      "Epoch 16, Loss: 0.6027, Accuracy: 0.6822\n",
      "Epoch 17, Loss: 0.6029, Accuracy: 0.6574\n",
      "Epoch 18, Loss: 0.6029, Accuracy: 0.6834\n",
      "Epoch 19, Loss: 0.6034, Accuracy: 0.6845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = np.load('ParticleNet_jetClassification\\\\QG_jets_1.npz')\n",
    "node_features = data['X']  # Shape: (100000, M, 4)\n",
    "labels = data['y']  # Shape: (100000,)\n",
    "\n",
    "# Define a function to create edges (fully connected graph for simplicity)\n",
    "def create_edge_index(num_nodes):\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:  # Avoid self-loops\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "    return torch.tensor([row, col], dtype=torch.long)\n",
    "\n",
    "# Convert dataset into a list of graphs\n",
    "graphs = []\n",
    "for i in range(len(node_features)):\n",
    "    x = torch.tensor(node_features[i], dtype=torch.float)  # Node features for jet i\n",
    "    y = torch.tensor(labels[i], dtype=torch.long)  # Label for jet i\n",
    "    edge_index = create_edge_index(x.shape[0])  # Fully connected graph\n",
    "    graphs.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 32\n",
    "loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 2: Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer for graph-level classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features into graph-level features\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 3: Initialize the model, optimizer, and loss function\n",
    "input_dim = 4  # Each particle has 4 features: pt, rapidity, azimuthal angle, pdgid\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Binary classification: quark (1) or gluon (0)\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)  # Output shape: (batch_size, output_dim)\n",
    "        loss = criterion(out, batch.y)  # batch.y shape: (batch_size,)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        _, pred = out.max(dim=1)\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Reduce epochs for testing; increase as needed\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gcn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model locally\n",
    "torch.save(model.state_dict(), 'gcn_model.pth')  # Save only the model's parameters\n",
    "print(\"Model saved to gcn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from gcn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Laod the model for inference or further training\n",
    "model_loaded = GCN(input_dim, hidden_dim, output_dim)  # Initialize the model architecture\n",
    "model.load_state_dict(torch.load('gcn_model.pth'))  # Load the saved parameters\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded from gcn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`numpy`**: Used to load and manipulate the `.npz` dataset.\n",
    "2. **`torch`**: Core PyTorch library for defining and training models.\n",
    "3. **`torch_geometric.data.Data`**: Represents a single graph with nodes, edges, and features.\n",
    "4. **`torch_geometric.data.DataLoader`**: Handles batching and shuffling of multiple graphs.\n",
    "5. **`torch_geometric.nn.GCNConv`**: Implements a Graph Convolutional Network (GCN) layer.\n",
    "6. **`torch_geometric.nn.global_mean_pool`**: Aggregates node features into a single graph-level representation.\n",
    "7. **`torch.nn.functional`**: Provides activation functions like `relu` and `log_softmax`.\n",
    "\n",
    "\n",
    "### **Step 1: Load the Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('ParticleNet_jetClassification\\\\QG_jets_1.npz')\n",
    "node_features = data['X']  # Shape: (100000, M, 4)\n",
    "labels = data['y']  # Shape: (100000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **`np.load`**: Loads the `.npz` file containing the dataset.\n",
    "9. **`node_features`**: Extracts the particle-level features for each jet. Shape `(100000, M, 4)`:\n",
    "   - `100000`: Number of jets.\n",
    "   - `M`: Maximum number of particles in a jet (zero-padded).\n",
    "   - `4`: Features per particle (`pt`, `rapidity`, `azimuthal angle`, `pdgid`).\n",
    "10. **`labels`**: Extracts the jet-level labels (`0` for gluon, `1` for quark). Shape `(100000,)`.\n",
    "\n",
    "### **Define a Function to Create Edges**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index(num_nodes):\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:  # Avoid self-loops\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "    return torch.tensor([row, col], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **`create_edge_index`**: Creates a fully connected graph for a jet:\n",
    "    - **`row` and `col`**: Define the source and target nodes for each edge.\n",
    "    - **`i != j`**: Avoids self-loops (edges from a node to itself). (Doing this might name the node, lose it's own context and just consider that of the edges) (We can explore if keeping the context i.e. allowing slef loops and observing how that affects the accuracy of the model, thus making cool discoveries about the perticles constituing qluon and quark jets, their properties as individual particles and as a gorup )\n",
    "12. **`torch.tensor`**: Converts the edge list into a PyTorch tensor of shape `(2, num_edges)`.\n",
    "\n",
    "### **Convert Dataset into a List of Graphs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for i in range(len(node_features)):\n",
    "    x = torch.tensor(node_features[i], dtype=torch.float)  # Node features for jet i\n",
    "    y = torch.tensor(labels[i], dtype=torch.long)  # Label for jet i\n",
    "    edge_index = create_edge_index(x.shape[0])  # Fully connected graph\n",
    "    graphs.append(Data(x=x, edge_index=edge_index, y=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **`graphs`**: A list to store graph representations of each jet.\n",
    "14. **`x`**: Converts the particle features of jet `i` into a PyTorch tensor.\n",
    "15. **`y`**: Converts the label of jet `i` into a PyTorch tensor.\n",
    "16. **`edge_index`**: Creates a fully connected graph for the particles in jet `i`.\n",
    "17. **`Data`**: Combines `x`, `edge_index`, and `y` into a graph object.\n",
    "18. **`graphs.append`**: Adds the graph to the list.\n",
    "\n",
    "### **Create a DataLoader for Batching**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. **`batch_size`**: Number of graphs (jets) per batch.\n",
    "20. **`DataLoader`**: Handles batching and shuffling of the graphs for training.\n",
    "\n",
    "### **Step 2: Define the GCN Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer for graph-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. **`GCN`**: Defines the Graph Convolutional Network model.\n",
    "22. **`GCNConv`**: Two GCN layers for learning node-level features.\n",
    "23. **`torch.nn.Linear`**: A fully connected layer for graph-level classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features into graph-level features\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. **`forward`**: Defines the forward pass of the model.\n",
    "25. **`data.x`**: Node features.\n",
    "26. **`data.edge_index`**: Graph structure (edges).\n",
    "27. **`data.batch`**: Batch information (which nodes belong to which graph).\n",
    "28. **`global_mean_pool`**: Aggregates node features into a single graph-level feature.\n",
    "29. **`F.log_softmax`**: Outputs probabilities for each class (quark or gluon).\n",
    "\n",
    "### **Step 3: Initialize the Model, Optimizer, and Loss Function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4  # Each particle has 4 features: pt, rapidity, azimuthal angle, pdgid\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Binary classification: quark (1) or gluon (0)\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. **`input_dim`**: Number of input features per particle.\n",
    "31. **`hidden_dim`**: Number of hidden units in the GCN layers.\n",
    "32. **`output_dim`**: Number of output classes (binary classification).\n",
    "33. **`Adam`**: Optimizer for training the model.\n",
    "34. **`CrossEntropyLoss`**: Loss function for classification tasks.\n",
    "\n",
    "### **Step 4: Train the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)  # Output shape: (batch_size, output_dim)\n",
    "        loss = criterion(out, batch.y)  # batch.y shape: (batch_size,)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. **`train`**: Defines the training loop.\n",
    "36. **`model.train()`**: Sets the model to training mode.\n",
    "37. **`optimizer.zero_grad()`**: Clears gradients from the previous step.\n",
    "38. **`model(batch)`**: Runs the forward pass for the batch.\n",
    "39. **`loss.backward()`**: Computes gradients for the model parameters.\n",
    "40. **`optimizer.step()`**: Updates the model parameters.\n",
    "41. **`total_loss`**: Accumulates the loss for the batch.\n",
    "\n",
    "\n",
    "### **Step 5: Evaluate the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        _, pred = out.max(dim=1)\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. **`test`**: Defines the evaluation loop.\n",
    "43. **`model.eval()`**: Sets the model to evaluation mode.\n",
    "44. **`out.max(dim=1)`**: Gets the predicted class for each graph.\n",
    "45. **`pred.eq(batch.y)`**: Compares predictions with ground truth labels.\n",
    "46. **`correct`**: Counts the number of correct predictions.\n",
    "47. **`total`**: Tracks the total number of graphs.\n",
    "\n",
    "\n",
    "### **Training Loop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # Reduce epochs for testing; increase as needed\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. **`range(20)`**: Runs the training loop for 20 epochs.\n",
    "49. **`train()`**: Trains the model for one epoch.\n",
    "50. **`test()`**: Evaluates the model after each epoch.\n",
    "51. **`print`**: Displays the loss and accuracy for each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_losses\u001b[49m) + \u001b[32m1\u001b[39m), train_losses, label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m plt.ylabel(\u001b[33m'\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# As we can see, in the code above, i missed adding the train_losses and val_accuracies lists to\n",
    "# store the training loss and validation accuracy for each epoch.\n",
    "# We shall not forget this in the next model training.\n",
    "\n",
    "# Filepath to the GCN output file\n",
    "file_path = 'GCN_output.txt'\n",
    "\n",
    "# Initialize empty lists to store training losses and validation accuracies\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Read the file and extract the values\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components\n",
    "        parts = line.strip().split(\", \")\n",
    "        if len(parts) == 3:  # Ensure the line has the expected format\n",
    "            # Extract the loss and accuracy values\n",
    "            loss = float(parts[1].split(\": \")[1])  # Extract the value after \"Loss: \"\n",
    "            accuracy = float(parts[2].split(\": \")[1])  # Extract the value after \"Accuracy: \"\n",
    "            \n",
    "            # Append the values to the respective lists\n",
    "            train_losses.append(loss)\n",
    "            val_accuracies.append(accuracy)\n",
    "\n",
    "# Print the extracted lists\n",
    "print(\"Train_losses:\", train_losses)\n",
    "print(\"val_accuracies:\", val_accuracies)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Evaluate Performance\n",
    "1. **Training Loss**:\n",
    "   - Monitor the loss printed during training. It should decrease steadily over epochs.\n",
    "   - If the loss stagnates or increases, consider adjusting the learning rate or model architecture.\n",
    "\n",
    "2. **Accuracy**:\n",
    "   - The accuracy printed during each epoch reflects how well the model is performing on the dataset.\n",
    "   - If the accuracy is low (e.g., close to 50%), the model might not be learning effectively.\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - If the training loss decreases but the accuracy does not improve, the model might be overfitting.\n",
    "   - To address this, consider adding regularization (e.g., dropout) or using a validation set.\n",
    "\n",
    "\n",
    "### Tips to Improve Performance\n",
    "1. **Edge Definition**:\n",
    "   - The current implementation uses a fully connected graph for simplicity. Defining edges based on physics-based criteria (e.g., Î”R between particles) might improve performance.\n",
    "   - Maybe by including self-loops, we might get better results\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Experiment with different values for:\n",
    "     - Learning rate (`lr`): Try values like `0.001`, `0.005`, or `0.01`.\n",
    "     - Hidden dimension (`hidden_dim`): Increase or decrease the number of hidden units (e.g., `32`, `128`).\n",
    "     - Batch size (`batch_size`): Try smaller or larger batch sizes (e.g., `16`, `64`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT - Code, Explaination and Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations Made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code for training a **Graph Attention Network (GAT)** on the quark/gluon jet classification dataset incorporates several considerations to ensure the model is trained effectively and the results are saved for later use. Below is a detailed breakdown of the considerations made in the code:\n",
    "\n",
    "(The considerations made here are very similar to those made for GCN)\n",
    "\n",
    "### **1. Dataset Preparation**\n",
    "#### **Graph Representation**\n",
    "- **Nodes**:\n",
    "  - Each particle in a jet is treated as a node, with features like `pt`, `rapidity`, `azimuthal angle`, and `pdgid`.\n",
    "  - The node features are extracted from the dataset (`X`) and converted into PyTorch tensors.\n",
    "- **Edges**:\n",
    "  - A fully connected graph is created for each jet, where all particles are connected to each other.\n",
    "  - This is implemented using the `create_edge_index` function, which avoids self-loops (`i != j`).\n",
    "  - **Consideration**: Fully connected graphs are simple but may introduce noise. Physics-based edge definitions (e.g., based on Î”R) could improve performance.\n",
    "\n",
    "#### **Batching**\n",
    "- The dataset is converted into a list of graphs using `torch_geometric.data.Data`, where each graph represents a jet.\n",
    "- A `DataLoader` is used to batch multiple graphs together for efficient training.\n",
    "- **Consideration**: The batch size is set to `32`, which balances computational efficiency and memory usage.\n",
    "\n",
    "\n",
    "### **2. Model Architecture**\n",
    "#### **Graph Attention Network (GAT)**\n",
    "- **Attention Mechanism**:\n",
    "  - The GAT layers (`GATConv`) dynamically assign weights to edges, allowing the model to focus on the most important relationships between particles.\n",
    "- **Multiple Attention Heads**:\n",
    "  - The first GAT layer uses `4` attention heads (`heads=4`) to capture diverse relationships between particles.\n",
    "  - The outputs of the attention heads are concatenated (`concat=True`).\n",
    "- **Graph-Level Pooling**:\n",
    "  - After the GAT layers, `global_mean_pool` is used to aggregate node features into a single graph-level representation.\n",
    "  - This ensures that the model outputs one prediction per graph (jet).\n",
    "- **Fully Connected Layer**:\n",
    "  - A fully connected layer maps the graph-level features to the output dimension (`2` for binary classification: quark or gluon).\n",
    "- **Activation Function**:\n",
    "  - The Exponential Linear Unit (ELU) activation function is used after each GAT layer to introduce non-linearity.\n",
    "\n",
    "\n",
    "### **3. Training and Evaluation**\n",
    "#### **Training Loop**\n",
    "- The model is trained for `20` epochs, with the following steps in each epoch:\n",
    "  - **Forward Pass**: The model processes the batch of graphs and outputs predictions.\n",
    "  - **Loss Calculation**: The cross-entropy loss is computed between the predictions and the true labels.\n",
    "  - **Backward Pass**: Gradients are computed, and the model parameters are updated using the Adam optimizer.\n",
    "- **Consideration**: The learning rate is set to `0.01`, and weight decay (`5e-4`) is used to regularize the model and prevent overfitting.\n",
    "\n",
    "#### **Evaluation**\n",
    "- After each epoch, the model is evaluated on the validation set:\n",
    "  - The accuracy is computed as the ratio of correct predictions to the total number of graphs.\n",
    "- **Consideration**: The evaluation ensures that the model is generalizing well to unseen data.\n",
    "\n",
    "### **4. Metric Tracking**\n",
    "- **Training Loss**:\n",
    "  - The loss for each epoch is stored in the `train_losses` list.\n",
    "- **Validation Accuracy**:\n",
    "  - The accuracy for each epoch is stored in the `val_accuracies` list.\n",
    "- **Consideration**: Tracking these metrics allows for performance analysis and visualization after training.\n",
    "\n",
    "### **5. Saving Results**\n",
    "#### **Model Parameters**\n",
    "- The trained model's parameters are saved to a file (`gan_model.pth`) using `torch.save`.\n",
    "- **Consideration**: Saving only the model's parameters (state dictionary) is more efficient and flexible than saving the entire model.\n",
    "\n",
    "#### **Training Metrics**\n",
    "- The `train_losses` and `val_accuracies` lists are saved to a `.txt` file (`training_metrics.txt`) for later analysis.\n",
    "- **Consideration**: Saving metrics ensures that the training process can be reviewed and visualized without rerunning the training.\n",
    "\n",
    "### **6. Scalability**\n",
    "- **Batching**:\n",
    "  - The use of a `DataLoader` ensures that the model can handle large datasets efficiently by processing them in batches.\n",
    "- **Memory Usage**:\n",
    "  - The batch size (`32`) and hidden dimension (`64`) are chosen to balance memory usage and computational efficiency.\n",
    "\n",
    "### **7. Domain-Specific Considerations**\n",
    "- **Physics-Based Features**:\n",
    "  - The node features (`pt`, `rapidity`, `azimuthal angle`, `pdgid`) are meaningful for distinguishing quark and gluon jets.\n",
    "- **Balanced Dataset**:\n",
    "  - The dataset is balanced (50% quark jets, 50% gluon jets), so no additional steps are needed for class balancing.\n",
    "\n",
    "### **8. Reproducibility**\n",
    "- **Saving Results**:\n",
    "  - The model parameters and training metrics are saved, ensuring that the results can be reproduced and analyzed later.\n",
    "- **Logging**:\n",
    "  - The training loop prints the loss and accuracy for each epoch, providing a clear record of the training process.\n",
    "\n",
    "\n",
    "### **Summary of Considerations**\n",
    "| **Aspect**              | **Consideration**                                                                 |\n",
    "|--------------------------|-----------------------------------------------------------------------------------|\n",
    "| Dataset Representation   | Nodes represent particles; edges define relationships (fully connected graph).   |\n",
    "| Model Architecture       | GAT layers with attention mechanisms and multiple heads for better feature learning. |\n",
    "| Training Process         | Tracks training loss and validation accuracy for performance monitoring.          |\n",
    "| Regularization           | Weight decay (`5e-4`) prevents overfitting.                                       |\n",
    "| Metric Saving            | Saves training metrics (`train_losses`, `val_accuracies`) for later analysis.     |\n",
    "| Model Saving             | Saves model parameters (`gan_model.pth`) for reuse without retraining.            |\n",
    "| Scalability              | Uses batching and efficient graph representation to handle large datasets.        |\n",
    "| Domain Knowledge         | Leverages physics-based features (`pt`, `rapidity`, etc.) for meaningful learning.|\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GAT is working on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Graph Attention Network (GAT)** works on the quark/gluon jet classification dataset by leveraging **attention mechanisms** to dynamically assign importance to the relationships (edges) between particles (nodes) in a jet (graph). Here's a detailed explanation of how the GAT processes the data:\n",
    "\n",
    "### **1. Dataset Representation**\n",
    "The dataset is represented as a collection of graphs, where:\n",
    "- **Nodes**: Represent particles in a jet, with features like `pt`, `rapidity`, `azimuthal angle`, and `pdgid`.\n",
    "- **Edges**: Represent relationships between particles. For simplicity, a fully connected graph is used, meaning every particle is connected to every other particle in the jet.\n",
    "- **Graph-Level Labels**: Each graph (jet) is labeled as `0` (gluon) or `1` (quark).\n",
    "\n",
    "### **2. GAT Architecture**\n",
    "The GAT model consists of the following components:\n",
    "\n",
    "#### **a. GAT Layers (`GATConv`)**\n",
    "- **Purpose**: Transform node features by aggregating information from neighboring nodes, with attention weights determining the importance of each neighbor.\n",
    "- **How It Works**:\n",
    "  1. For each node, the GAT computes attention coefficients for all its neighbors based on their features.\n",
    "  2. These coefficients are normalized (using softmax) to ensure they sum to 1.\n",
    "  3. The node's new feature is computed as a weighted sum of its neighbors' features, where the weights are the attention coefficients.\n",
    "- **Multiple Attention Heads**:\n",
    "  - The first GAT layer uses multiple attention heads (`heads=4`) to capture diverse relationships between particles.\n",
    "  - The outputs of the attention heads are concatenated to form the updated node features.\n",
    "\n",
    "#### **b. Graph-Level Pooling (`global_mean_pool`)**\n",
    "- **Purpose**: Aggregate node features into a single graph-level representation.\n",
    "- **How It Works**:\n",
    "  - For each graph, the mean of all node features is computed. This ensures that the model outputs one feature vector per graph.\n",
    "\n",
    "#### **c. Fully Connected Layer**\n",
    "- **Purpose**: Map the graph-level features to the output dimension (`2` for binary classification: quark or gluon).\n",
    "- **How It Works**:\n",
    "  - The graph-level features are passed through a fully connected layer, which outputs the logits for each class.\n",
    "\n",
    "### **3. Forward Pass**\n",
    "The GAT processes the data in the following steps:\n",
    "\n",
    "#### **Step 1: Input**\n",
    "- **Node Features (`x`)**: Each node (particle) has 4 features: `pt`, `rapidity`, `azimuthal angle`, and `pdgid`.\n",
    "- **Edge Index (`edge_index`)**: Defines the graph structure (which nodes are connected).\n",
    "\n",
    "#### **Step 2: First GAT Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.gat1(x, edge_index)\n",
    "x = F.elu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - `x`: Node features (shape: `[num_nodes, input_dim]`).\n",
    "  - `edge_index`: Graph structure (edges).\n",
    "- **Operation**:\n",
    "  - The GAT layer computes attention coefficients for each edge and updates the node features based on the weighted sum of its neighbors' features.\n",
    "  - The updated features are passed through an ELU activation function to introduce non-linearity.\n",
    "- **Output**:\n",
    "  - Updated node features (shape: `[num_nodes, hidden_dim * heads]`).\n",
    "\n",
    "#### **Step 3: Second GAT Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.gat2(x, edge_index)\n",
    "x = F.elu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - Node features from the first GAT layer.\n",
    "- **Operation**:\n",
    "  - Similar to the first layer, but with a single attention head (`heads=1`) and no concatenation.\n",
    "- **Output**:\n",
    "  - Final node features (shape: `[num_nodes, hidden_dim]`).\n",
    "\n",
    "#### **Step 4: Graph-Level Pooling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = global_mean_pool(x, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - Node features (shape: `[num_nodes, hidden_dim]`).\n",
    "  - `batch`: A tensor that maps each node to its corresponding graph in the batch.\n",
    "- **Operation**:\n",
    "  - For each graph, the mean of the node features is computed to produce a single graph-level feature vector.\n",
    "- **Output**:\n",
    "  - Graph-level features (shape: `[batch_size, hidden_dim]`).\n",
    "\n",
    "#### **Step 5: Fully Connected Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.fc(x)\n",
    "return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - Graph-level features (shape: `[batch_size, hidden_dim]`).\n",
    "- **Operation**:\n",
    "  - The fully connected layer maps the graph-level features to the output dimension (`2` for binary classification).\n",
    "  - The `log_softmax` function converts the logits into log probabilities for each class.\n",
    "- **Output**:\n",
    "  - Predicted probabilities for each graph in the batch (shape: `[batch_size, 2]`).\n",
    "\n",
    "\n",
    "### **4. Training**\n",
    "During training, the model learns to minimize the **cross-entropy loss** between the predicted probabilities and the true labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(out, batch.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**:\n",
    "  - `out`: Predicted probabilities (shape: `[batch_size, 2]`).\n",
    "  - `batch.y`: True labels for each graph in the batch (shape: `[batch_size]`).\n",
    "- **Operation**:\n",
    "  - The loss function computes how far the predictions are from the true labels.\n",
    "- **Output**:\n",
    "  - A scalar loss value, which is used to update the model parameters.\n",
    "\n",
    "### **5. Evaluation**\n",
    "During evaluation, the model predicts the class for each graph and computes the accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = out.max(dim=1)\n",
    "correct += pred.eq(batch.y).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`out.max(dim=1)`**: Gets the predicted class (index of the maximum probability).\n",
    "- **`pred.eq(batch.y)`**: Compares predictions with true labels.\n",
    "- **Accuracy**: The ratio of correct predictions to the total number of graphs.\n",
    "\n",
    "### **6. Why GAT Works for This Dataset**\n",
    "- **Dynamic Edge Weights**:\n",
    "  - The attention mechanism allows the GAT to assign different importance to different edges, enabling it to focus on the most relevant particle relationships.\n",
    "- **Multiple Attention Heads**:\n",
    "  - Multiple attention heads capture diverse relationships between particles, improving the model's ability to learn complex patterns.\n",
    "- **Graph-Level Pooling**:\n",
    "  - Aggregating node features into a graph-level representation ensures that the model outputs a single prediction for each jet.\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "The GAT processes the dataset by:\n",
    "1. Representing each jet as a graph with particles as nodes and their relationships as edges.\n",
    "2. Using attention mechanisms to dynamically assign importance to edges.\n",
    "3. Aggregating node features into a graph-level representation.\n",
    "4. Predicting the class (quark or gluon) for each graph.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmuno\\OneDrive\\Desktop\\GSOC25\\GSOC25\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0182, Accuracy: 0.6235\n",
      "Epoch 1, Loss: 0.7033, Accuracy: 0.5001\n",
      "Epoch 2, Loss: 0.9849, Accuracy: 0.5001\n",
      "Epoch 3, Loss: 0.7288, Accuracy: 0.5000\n",
      "Epoch 4, Loss: 0.8144, Accuracy: 0.5000\n",
      "Epoch 5, Loss: 0.7537, Accuracy: 0.5000\n",
      "Epoch 6, Loss: 0.7915, Accuracy: 0.5000\n",
      "Epoch 7, Loss: 0.8520, Accuracy: 0.5000\n",
      "Epoch 8, Loss: 0.9297, Accuracy: 0.5000\n",
      "Epoch 9, Loss: 0.7117, Accuracy: 0.5000\n",
      "Epoch 10, Loss: 1.0586, Accuracy: 0.5893\n",
      "Epoch 11, Loss: 0.7047, Accuracy: 0.5000\n",
      "Epoch 12, Loss: 0.7265, Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = np.load('ParticleNet_jetClassification\\\\QG_jets_1.npz')\n",
    "node_features = data['X']  # Shape: (100000, M, 4)\n",
    "labels = data['y']  # Shape: (100000,)\n",
    "\n",
    "# Define a function to create edges (fully connected graph for simplicity)\n",
    "def create_edge_index(num_nodes):\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:  # Avoid self-loops\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "    return torch.tensor([row, col], dtype=torch.long)\n",
    "\n",
    "# Convert dataset into a list of graphs\n",
    "graphs = []\n",
    "for i in range(len(node_features)):\n",
    "    x = torch.tensor(node_features[i], dtype=torch.float)  # Node features for jet i\n",
    "    y = torch.tensor(labels[i], dtype=torch.long)  # Label for jet i\n",
    "    edge_index = create_edge_index(x.shape[0])  # Fully connected graph\n",
    "    graphs.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 32\n",
    "loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 2: Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer for graph-level classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)  # Use ELU activation for GAT\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features into graph-level features\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 3: Initialize the model, optimizer, and loss function\n",
    "input_dim = 4  # Each particle has 4 features: pt, rapidity, azimuthal angle, pdgid\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Binary classification: quark (1) or gluon (0)\n",
    "\n",
    "model = GAT(input_dim, hidden_dim, output_dim, heads=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)  # Output shape: (batch_size, output_dim)\n",
    "        loss = criterion(out, batch.y)  # batch.y shape: (batch_size,)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        _, pred = out.max(dim=1)\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Adjust the number of epochs as needed\n",
    "    loss = train()  # Train the model for one epoch\n",
    "    acc = test()  # Evaluate the model on the validation set\n",
    "    train_losses.append(loss)  # Store the training loss\n",
    "    val_accuracies.append(acc)  # Store the validation accuracy\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "# Save the trained model locally\n",
    "torch.save(model.state_dict(), 'gan_model.pth')  # Save only the model's parameters\n",
    "print(\"Model saved to gan_model.pth\")\n",
    "\n",
    "# Save train_losses and val_accuracies to a .txt file\n",
    "with open('training_metrics.txt', 'w') as f:\n",
    "    f.write(\"Train Losses:\\n\")\n",
    "    f.write(\", \".join(map(str, train_losses)) + \"\\n\")\n",
    "    f.write(\"Validation Accuracies:\\n\")\n",
    "    f.write(\", \".join(map(str, val_accuracies)) + \"\\n\")\n",
    "\n",
    "print(\"Training metrics saved to training_metrics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`numpy`**: Used to load and manipulate the `.npz` dataset.\n",
    "2. **`torch`**: Core PyTorch library for defining and training models.\n",
    "3. **`torch_geometric.data.Data`**: Represents a single graph with nodes, edges, and features.\n",
    "4. **`torch_geometric.data.DataLoader`**: Handles batching and shuffling of multiple graphs.\n",
    "5. **`torch_geometric.nn.GATConv`**: Implements the Graph Attention Network (GAT) layer.\n",
    "6. **`torch_geometric.nn.global_mean_pool`**: Aggregates node features into a single graph-level representation.\n",
    "7. **`torch.nn.functional`**: Provides activation functions like `elu` and `log_softmax`.\n",
    "\n",
    "### **Step 1: Load the Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('ParticleNet_jetClassification\\\\QG_jets_1.npz')\n",
    "node_features = data['X']  # Shape: (100000, M, 4)\n",
    "labels = data['y']  # Shape: (100000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **`np.load`**: Loads the `.npz` file containing the dataset.\n",
    "9. **`node_features`**: Extracts the particle-level features for each jet. Shape `(100000, M, 4)`:\n",
    "   - `100000`: Number of jets.\n",
    "   - `M`: Maximum number of particles in a jet (zero-padded).\n",
    "   - `4`: Features per particle (`pt`, `rapidity`, `azimuthal angle`, `pdgid`).\n",
    "10. **`labels`**: Extracts the jet-level labels (`0` for gluon, `1` for quark). Shape `(100000,)`.\n",
    "\n",
    "### **Define a Function to Create Edges**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index(num_nodes):\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:  # Avoid self-loops\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "    return torch.tensor([row, col], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **`create_edge_index`**: Creates a fully connected graph for a jet:\n",
    "    - **`row` and `col`**: Define the source and target nodes for each edge.\n",
    "    - **`i != j`**: Avoids self-loops (edges from a node to itself).\n",
    "12. **`torch.tensor`**: Converts the edge list into a PyTorch tensor of shape `(2, num_edges)`.\n",
    "\n",
    "### **Convert Dataset into a List of Graphs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for i in range(len(node_features)):\n",
    "    x = torch.tensor(node_features[i], dtype=torch.float)  # Node features for jet i\n",
    "    y = torch.tensor(labels[i], dtype=torch.long)  # Label for jet i\n",
    "    edge_index = create_edge_index(x.shape[0])  # Fully connected graph\n",
    "    graphs.append(Data(x=x, edge_index=edge_index, y=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **`graphs`**: A list to store graph representations of each jet.\n",
    "14. **`x`**: Converts the particle features of jet `i` into a PyTorch tensor.\n",
    "15. **`y`**: Converts the label of jet `i` into a PyTorch tensor.\n",
    "16. **`edge_index`**: Creates a fully connected graph for the particles in jet `i`.\n",
    "17. **`Data`**: Combines `x`, `edge_index`, and `y` into a graph object.\n",
    "18. **`graphs.append`**: Adds the graph to the list.\n",
    "\n",
    "\n",
    "### **Create a DataLoader for Batching**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "loader = DataLoader(graphs, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. **`batch_size`**: Number of graphs (jets) per batch.\n",
    "20. **`DataLoader`**: Handles batching and shuffling of the graphs for training.\n",
    "\n",
    "### **Step 2: Define the GAT Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer for graph-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. **`GAT`**: Defines the Graph Attention Network model.\n",
    "22. **`GATConv`**: Two GAT layers for learning node-level features.\n",
    "23. **`torch.nn.Linear`**: A fully connected layer for graph-level classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)  # Use ELU activation for GAT\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features into graph-level features\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. **`forward`**: Defines the forward pass of the model.\n",
    "25. **`data.x`**: Node features.\n",
    "26. **`data.edge_index`**: Graph structure (edges).\n",
    "27. **`data.batch`**: Batch information (which nodes belong to which graph).\n",
    "28. **`global_mean_pool`**: Aggregates node features into a single graph-level feature.\n",
    "29. **`F.log_softmax`**: Outputs probabilities for each class (quark or gluon).\n",
    "\n",
    "### **Step 3: Initialize the Model, Optimizer, and Loss Function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4  # Each particle has 4 features: pt, rapidity, azimuthal angle, pdgid\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Binary classification: quark (1) or gluon (0)\n",
    "\n",
    "model = GAT(input_dim, hidden_dim, output_dim, heads=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. **`input_dim`**: Number of input features per particle.\n",
    "31. **`hidden_dim`**: Number of hidden units in the GAT layers.\n",
    "32. **`output_dim`**: Number of output classes (binary classification).\n",
    "33. **`Adam`**: Optimizer for training the model.\n",
    "34. **`CrossEntropyLoss`**: Loss function for classification tasks.\n",
    "### **Step 4: Train the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)  # Output shape: (batch_size, output_dim)\n",
    "        loss = criterion(out, batch.y)  # batch.y shape: (batch_size,)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. **`train`**: Defines the training loop.\n",
    "36. **`model.train()`**: Sets the model to training mode.\n",
    "37. **`optimizer.zero_grad()`**: Clears gradients from the previous step.\n",
    "38. **`model(batch)`**: Runs the forward pass for the batch.\n",
    "39. **`loss.backward()`**: Computes gradients for the model parameters.\n",
    "40. **`optimizer.step()`**: Updates the model parameters.\n",
    "41. **`total_loss`**: Accumulates the loss for the batch.\n",
    "\n",
    "### **Step 5: Evaluate the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        out = model(batch)\n",
    "        _, pred = out.max(dim=1)\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. **`test`**: Defines the evaluation loop.\n",
    "43. **`model.eval()`**: Sets the model to evaluation mode.\n",
    "44. **`out.max(dim=1)`**: Gets the predicted class for each graph.\n",
    "45. **`pred.eq(batch.y)`**: Compares predictions with ground truth labels.\n",
    "46. **Accuracy**: The ratio of correct predictions to the total number of graphs.\n",
    "\n",
    "### **Training Loop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # Adjust the number of epochs as needed\n",
    "    loss = train()  # Train the model for one epoch\n",
    "    acc = test()  # Evaluate the model on the validation set\n",
    "    train_losses.append(loss)  # Store the training loss\n",
    "    val_accuracies.append(acc)  # Store the validation accuracy\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. **`range(20)`**: Runs the training loop for 20 epochs.\n",
    "48. **`train()`**: Trains the model for one epoch.\n",
    "49. **`test()`**: Evaluates the model after each epoch.\n",
    "50. **`train_losses`**: Stores the training loss for each epoch.\n",
    "51. **`val_accuracies`**: Stores the validation accuracy for each epoch.\n",
    "\n",
    "\n",
    "### **Saving the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gan_model.pth')  # Save only the model's parameters\n",
    "print(\"Model saved to gan_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52. **`torch.save`**: Saves the model's parameters to a file (`gan_model.pth`).\n",
    "\n",
    "\n",
    "### **Saving Metrics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_metrics.txt', 'w') as f:\n",
    "    f.write(\"Train Losses:\\n\")\n",
    "    f.write(\", \".join(map(str, train_losses)) + \"\\n\")\n",
    "    f.write(\"Validation Accuracies:\\n\")\n",
    "    f.write(\", \".join(map(str, val_accuracies)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53. **`open`**: Opens a file to save the training metrics.\n",
    "54. **`map(str, train_losses)`**: Converts the loss values to strings for saving.\n",
    "55. **`f.write`**: Writes the metrics to the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSOC25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
